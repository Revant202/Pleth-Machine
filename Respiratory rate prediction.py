# -*- coding: utf-8 -*-
"""ML CODE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1amjN9uz7pzPxFBiMd6eWSzRgWDLrRgDB
"""

import requests
from os import listdir, mkdir
from os.path import isdir, isfile, join
import numpy as np
import pandas as pd
from scipy.stats import skew, kurtosis
from tqdm import tqdm

base = 'https://archive.physionet.org/physiobank/database/bidmc/bidmc_csv/'
endings = ['_Breaths.csv','_Numerics.csv','_Signals.csv','_Fix.txt']

files = {}

nums = []
for n in range(1,54):
    if n<10:
        nums.append('0'+str(n))
    else:
        nums.append(str(n))
        
for n in nums:
    files[n] = ['bidmc_'+n+ending for ending in endings]

csv = {}
txt = {}
for number in tqdm(files.keys()):
    csv[number] = []
    for f in files[number]:
        if 'csv' in f:
            csv[number].append(base+f)
        else:
            txt[number] = f

for f in tqdm(files.keys()):
    for n in files[f]:
        if '.txt' not in n:
            df = pd.read_csv(base+n)
            if not isdir("csv"):
                mkdir("csv")
            df.to_csv('csv/'+n)

for n in tqdm(txt.keys()): 
    text = requests.get(base+txt[n])
    text = text.text
    if not isdir("txt"):
        mkdir("txt")
    with open('txt/'+txt[n], "w") as text_file:
        text_file.write(text)

def make_dataframe(num):
    signals= pd.read_csv('csv/bidmc_'+str(num)+'_Signals.csv',index_col=0)
    signals['sec'] = signals['Time [s]'].apply(lambda x: int(np.floor(x)))
    numerics = pd.read_csv('csv/bidmc_'+str(num)+'_Numerics.csv',index_col=0)
    numerics.fillna(numerics.mean(),inplace=True) 
    numerics.rename(columns={'Time [s]':'sec'},inplace=True)
    numerics.drop(' RESP',axis=1,inplace=True)
    numerics['sec'] = numerics['sec'].apply(lambda x: int(x))
    signals = signals[[' RESP', ' PLETH', ' V', ' AVR', ' II','sec','Time [s]']]
    person = signals.merge(numerics,on='sec',how='outer')
    Hz_125_cols = [' RESP', ' PLETH', ' V', ' AVR', ' II']
    Min = person[Hz_125_cols+['sec']].groupby('sec').min()
    Min.columns = [i+'_Min' for i in Min.columns]
    Max = person[Hz_125_cols+['sec']].groupby('sec').max()
    Max.columns = [i+'_Max' for i in Max.columns]
    Mean = person[Hz_125_cols+['sec']].groupby('sec').mean()
    Mean.columns = Mean.columns = [i+'_Mean' for i in Mean.columns]
    Kurt = person[Hz_125_cols+['sec']].groupby('sec').agg(lambda x: kurtosis(x))
    Kurt.columns = [i+'_Kurt' for i in Kurt.columns]
    Skw = person[Hz_125_cols+['sec']].groupby('sec').agg(lambda x: skew(x))
    Skw.columns = [i+'_Skw' for i in Skw.columns]
    summary_frames = [Min,Max,Mean,Kurt,Skw]
    one_sec_summary = pd.concat(summary_frames,axis=1).reset_index()
    person = person.merge(one_sec_summary,on='sec',how='outer')
    if not isdir("person_csvs"):
        mkdir("person_csvs")
    person.to_csv('person_csvs/person_'+str(num)+'.csv')

nums = []
for n in range(1,54):
    if n<10:
        nums.append('0'+str(n))
    else:
        nums.append(str(n))

for number in nums:
    try:
        make_dataframe(number)
    except:
        print("Ignoring person", number, "due to error")

onlyfiles = [f for f in listdir('person_csvs') if isfile(join('person_csvs', f))]
person_files = ['person_csvs/'+i for i in onlyfiles if 'person' in i]
files = []
for person in tqdm(person_files):
    df = pd.read_csv(person,index_col=0)
    files.append(df)

df = pd.concat(files, axis=0, ignore_index=True)
df.dropna(inplace=True)

print(df.shape)

df.to_csv('person_csvs/all_people.csv')

df.head()

df.skew()

df.kurtosis()

df.var()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('person_csvs/person_13.csv',index_col=0)
resp = [i for i in data.columns if 'RESP' in i and i!=' RESP']
data.drop(resp+['Time [s]','sec'],axis=1,inplace=True)
data.head()

df.mean()

df.median()

df.quantile(0.25)

df.quantile(0.5)

df.quantile(0.75)

df.std()

df.corr()

import seaborn as sns
plt.xticks(weight="bold")
plt.yticks(weight="bold")
plt.grid()
sns.boxplot(df[' RESP']).set_fontsize('18')

plt.show()

plt.xticks(weight="bold")
plt.yticks(weight="bold")
plt.grid()

sns.distplot(df[' RESP'],hist=None)
plt.show()

plt.xticks(weight="bold")
plt.yticks(weight="bold")
plt.grid()
sns.histplot(df[' RESP'])
plt.show()

from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
X = data.drop(' RESP',axis=1)
SS.fit(X,y=None)
y = data[' RESP'].values
X = SS.transform(X)

pip install catboost

import time
import numpy as np
from xgboost import XGBRegressor
import catboost
from catboost import CatBoostRegressor
from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error
from sklearn.linear_model import Lasso,Ridge,ElasticNet, BayesianRidge, LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn import neighbors
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

models = {'DT':DecisionTreeRegressor(),'ElasticNet':ElasticNet(),
         'KNN':neighbors.KNeighborsRegressor(),
         'rff':RandomForestRegressor(), 'xgb':XGBRegressor(),"CB":CatBoostRegressor()}




def model_performance(X,y):
    times =[]
    keys = []
    mean_squared_errors = []
    mean_abs_error = []
    R2_scores = []
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=10)

    for k,v in models.items():
        model = v
        t0=time.time()
        model.fit(X_train, y_train)
        train_time = time.time()-t0
        t1 = time.time()
        pred = model.predict(X_test)
        predict_time = time.time()-t1
        pred = pd.Series(pred)
        Time_total = train_time+predict_time
        times.append(Time_total)
        R2_scores.append(r2_score(y_test,pred))
        mean_squared_errors.append(mean_squared_error(y_test,pred))
        mean_abs_error.append(mean_absolute_error(y_test,pred))
        keys.append(k)
    table = pd.DataFrame({'model':keys, 'RMSE':mean_squared_errors,'MAE':mean_abs_error,'R2 score':R2_scores,'time':times})
    table['RMSE'] = table['RMSE'].apply(lambda x: np.sqrt(x))
    return table

model_performance(X,y)

X = data.drop(' RESP',axis=1)

SS = StandardScaler()
X = data.drop(' RESP', axis=1)
columns = X.columns
SS.fit(X, y=None)
y = data[' RESP'].values
X = SS.transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)
# Uncomment this line and comment the two below it if you wish to load a model
# model = load('saved_models/RFF_joblib')
model = RandomForestRegressor()
model.fit(X_train, y_train)

y_pred_train = model.predict(X_train)
y_pred = model.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)
print('MSE:', MSE)
print('MAE:',MAE)
print('r2:',R2)

plt.rcParams.update({'font.size':20})
plt.scatter(y_test, y_pred, color = 'blue')
plt.xticks(weight="bold")
plt.yticks(weight="bold")
plt.legend(["r-squared = {:.3f}".format(r2_score(y_test, y_pred))],bbox_to_anchor =(0.7, 1.25), ncol = 2)
plt.grid()
plt.show()

regressor = DecisionTreeRegressor()
regressor.fit(X_train, y_train)
y_pred_train = regressor.predict(X_train)
y_pred = regressor.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)
print('MSE:', MSE)
print('MAE:',MAE)
print('r2:',R2)

plt.rcParams.update({'font.size':20})
plt.scatter(y_test, y_pred, color = 'blue')
plt.xticks(weight="bold")
plt.yticks(weight="bold")
plt.legend(["r-squared = {:.3f}".format(r2_score(y_test, y_pred))],bbox_to_anchor =(0.7, 1.25), ncol = 2)
plt.grid()
plt.show()

cregressor = CatBoostRegressor()
cregressor.fit(X_train, y_train)
y_pred_train = cregressor.predict(X_train)
y_pred = cregressor.predict(X_test)
MSE = mean_squared_error(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred)
R2 = r2_score(y_test, y_pred)
print('MSE:', MSE)
print('MAE:',MAE)
print('r2:',R2)

plt.rcParams.update({'font.size':20})
plt.scatter(y_test, y_pred, color = 'blue')
plt.xticks(weight="bold")
plt.yticks(weight="bold")
plt.legend(["r-squared = {:.3f}".format(r2_score(y_test, y_pred))],bbox_to_anchor =(0.7, 1.25), ncol = 2)
plt.grid()
plt.show()

X = pd.DataFrame(X, columns=columns)

importances = model.feature_importances_
std = np.std([model.feature_importances_ for tree in model.estimators_], axis=0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")
feature_importances = []
for f in range(X.shape[1]):
    print("%d. %s (%f)" % (f + 1, X.columns[indices[f]], importances[indices[f]]))
    feature_importances.append((X.columns[indices[f]], importances[indices[f]]))

values = [i[1] for i in feature_importances]
names = [i[0] for i in feature_importances]

# Plot the feature importances of the forest
import matplotlib.pyplot as plt
plt.figure(figsize=(20,12))
plt.title("Random Forest Feature Importances", fontsize=30)
plt.barh(range(len(names)), values, color="r", yerr=std[indices], align="center")
plt.yticks(range(len(names)), names, fontsize=15,weight="bold")
plt.xticks(np.linspace(0, max(importances), 30), rotation=45,weight="bold")
plt.show()

from xgboost import plot_importance
from xgboost import XGBRegressor
xgb=XGBRegressor()
xgb.fit(X_train, y_train)
plot_importance(xgb)
plt.show()

from sklearn.feature_selection import  SelectFromModel
selection = SelectFromModel(xgb, prefit=True)
select_X_train = selection.transform(X_train)
# train model
selection_model = XGBRegressor()
selection_model.fit(select_X_train, y_train)
# eval model
select_X_test = selection.transform(X_test)
y_pred = selection_model.predict(select_X_test)

params = {'gamma': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4, 200],
              'learning_rate': [0.01, 0.03, 0.06, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7],
              'max_depth': [5,6,7,8,9,10,11,12,13,14],
              'n_estimators': [50,65,80,100,115,130,150],
               "min_child_weight" : [ 1, 3, 5, 7 ],
                "colsample_bytree" : [ 0.3, 0.4, 0.5 ,0.55,0.6, 0.7 ],
              'reg_alpha': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4,200],
              'reg_lambda': [0,0.1,0.2,0.4,0.8,1.6,3.2,6.4,12.8,25.6,51.2,102.4,200]}

from sklearn.model_selection import RandomizedSearchCV
random_search=RandomizedSearchCV(xgb,param_distributions=params,n_iter=5,n_jobs=-1,cv=5,verbose=3, error_score = 'raise')
random_search.fit(X_train,y_train)